---
title: "ESG Content Analysis on US companies"
author: "QTA group 2"
date: "5/19/2021"
output: html_document
---
## Table of Content (temporary)
**Introduction**

**Sample selection and data collection**

**Research question**

**Readability and the nature of 10K fillings**

**Correspondence**

**Collocation Analysis**

```{r Tokenization, include = FALSE}

fashion_tokens <- fashion_corp %>%
  tokens(remove_punct = TRUE, # remove punctuation
         remove_numbers = TRUE, # remove numbers
         remove_symbols = TRUE,
         remove_url = TRUE) %>% # remove symbols
  tokens_tolower() # everything to lower cases
# takes some minutes!

words_to_be_removed <- c(stopwords("english"), "million", "fiscal", "january", "february",
           "march", "april", "may", "june", "july", "august", "september",
           "october", "november", "december", "business", "net", "s")

fashion_tokens <- fashion_tokens %>%
  tokens_remove(words_to_be_removed)
```


**Applied dictionaries**

**Relative ESG-wording**

**Comparison with ESG-Ratings**

**Trends between industries**

**Limitations**

```{r Loading packages and load/prepare 10Ks, include=FALSE}
# load packages
library(readtext)
library(dplyr)
library(here)
library(readxl)
library(quanteda)
library(quanteda.textmodels)
library(stringr)
library(tidyverse)
library(ggplot2)
library(plotly)

# load excel files with cik numbers for each industry with the companies
fashion <- read_excel(here("companies", "fashion_cik.xlsx"), sheet = "Sheet1", col_names = TRUE)
# energy <- read_excel()
# industry3 <- read_excel()

# keeping only the cik number
fashion_cik <- fashion[,13]
# energy_cik <- energy[,13]
# industry3_cik <- industry3[,13]

# load the 10Ks from downloaded files

fashion_10k <- NULL
energy_10k <- NULL
industry3_10k <- NULL

# loop that collects all reports for all companies for each industry
# this will take some minutes!!!

# fashion (should be 436 observations)
for (i in fashion_cik$CIK) {
  output <- readtext(here("Edgar filings_HTML view", "Form 10-K", i))
  fashion_10k <- rbind(fashion_10k, output)
}

# energy
for (i in energy_cik$CIK) {
  output <- readtext(here("Edgar filings_HTML view", "Form 10-K", i))
  fashion_10k <- rbind(all, output)
}

# industry3
for (i in industry_cik$CIK) {
  output <- readtext(here("Edgar filings_HTML view", "Form 10-K", i))
  fashion_10k <- rbind(all, output)
}

# simple cleaning
fashion_10k$text <- str_remove_all(fashion_10k$text, "\n•")
fashion_10k$text <- str_remove_all(fashion_10k$text, "\n")
#energy_10k
#industry3

# define columns into cik, report type and report year
fashion_10k <- separate(
  fashion_10k,
  1,
  into = c("cik", "type", "year", NA),
  sep = "_",
  remove = TRUE)

#energy
#industry3

# only keep year
#keep the year and not the full date
fashion_10k <- separate(
  fashion_10k,
  "year",
  into = c("year", NA),
  sep = "-",
  remove = TRUE)

#energy
#industry3
```


#### Introduction ####

#### Sample selection and data ####
We took the 10K filings back to the year 2004. This leads to 18 reports if the on from 2021 was already made public.For the fashion industry we have a average of `r round(ndoc(fashion_10k)/length(unique(fashion_10k$cik)), digits = 2)` total 10K filings per company. This is due to some later SEC registrations after 2004 for some companies.

#### Research question ####

#### Readability and the nature of 10K fillings ####
```{r Creating a corpus and apply the readability measures, include=FALSE}
# creating a corpus

fashion_corp <- corpus(fashion_10k)
#energy
#industry3


# apply Flesch readability measure (takes a few minutes)

fashion_rd_flesch <- fashion_corp %>%
  textstat_readability(measure = "Flesch", remove_hyphens = TRUE)
#energy
# industry3

# apply FOG readability measure (takes a few minutes)

fashion_rd_fog <- fashion_corp %>%
  textstat_readability(measure = "FOG", remove_hyphens = TRUE)
#energy
#industry3
```
//Flesch//

There are many different methods to try to measure the complexity of a underlying text source. Most measures take somehow into account the text structure, the length of sentences, the difficulty of used words and the usage of infrequent words. One of the first readability measures was developed by Rudolph Flesch back in 1948. As one of the oldest but still frequently used measure for readability it tries to reflect the complexity of a text by the average sentence length and the average number of syllables per word. The Flesch Reading Ease readability measure works with an index ranging from 0 to 100, with a higher index representing a simpler text to read. Theoretically a maximum score of 121.22 can be attended if every sentence just have a one-syllable word. There is also no lower limit. Therefore, some very complicated sentences can cause negative scores. With an average index of `r round(mean(fashion_rd_flesch$Flesch), digits = 2)` the 10Ks seem to be very complex based on the Flesch measure. A score below the score of 30 indicates a high difficulty to read the text and is best understood by university graduates. On the plot below the Flesch score for each company in our fashion sample across all years is shown. The readability scores tend to be more different between companies during the earlier years in our sample. From 2012 the companies have more similar reflected complexity as the scores vary much less as for the earlier ears. This can be due to more standardized restrictions by the SEC, but for the last two years 2020 and 2021 the scores do vary more again. Additionally, the complexity is increasing towards the present what is represented by a lower score. For the two last years multiple Flesch scores are below zero, suggesting these are very complicated texts to deal with. The higher variation and measured complexity could be caused by  COVID at how the firms are differently affected by the virus. Looking at the companies individually, it is noticeable that Lakeland Industries Inc. (cik = 798081, with an average Flesch score (AVS) of 5.81), Hansebrand Inc. (cik = 1359841, AVS = 7.65), PVH Corp. (cik = 78239, AVS = 9.06) and Carter Inc. (cik = 1060822, AVS = 9.37) tend to have more complex 10Ks. Whereas, the 10Ks from the companies G III Apparel Group LTD (cik = 821002, AVS = 19.92), Vera Bradley (cik = 1495320, AVS = 15.32) and Movado (cik = 72573, AVS = 14.34) are supposedly more easier to understand.

references:
- https://readabilityformulas.com/flesch-reading-ease-readability-formula.php


```{r Readability plots Flesch, echo = FALSE, fig.width = 10 , fig.fullwidth=TRUE}

# all companies, all years
ggplotly(ggplot(fashion_rd_flesch, aes(x=fashion_10k$year, y =Flesch, colour=fashion_10k$cik)) + geom_point() + ggtitle("Flesch - Readability for fashion companies"), tooltip = c("colour", "y", "x"))
```

//FOG//

Another well-known readability measure is the FOG index. It is widely used, also in the area of Accounting and Finance. Biddle, Hilary, and Verdi (2009) even stated that it is “a measure of financial statement readability”. The FOG index evaluates the difficulty of understanding a text on similar parameters like the Flesch index. Gunning-Fog, the creator of the FOG index used also the average sentence length. Together with the proportion of complex words (here words with 3-syllables) it forms a linear combination to measure the complexity of a underlying text source. In contrast to the Flesch score a higher FOG index indicates a more difficult text. A index of less than 8 is found to be understood by an universal audience, whereas a score above 17 can only be understood collage graduates. The average FOG score for our sample of 27 fashion companies from the US is `r round(mean(fashion_rd_fog$FOG), digits = 2)`. The 10Ks are therefore considered as very complex and difficult to read. The graph below displaying all readability scores for each fashion company over 18 years shows a similar picture than the one for the Flesch score. Keep in mind that the FOG scores behave the oppisite way than the Flesch scores, as a higher FOG score indicates a higher difficulty to read the text source. It can also be seen that the readability is lower towards the more recent years and the variance is slightly smaller during the years 2016 to 2019. However, it seems that for the FOG index the readability scores vary less, expect of single outliers. The company with the most complex 10Ks is PVH Corp. (cik = 78239, 26.11), which was the third complex one using the Flesch score. The most understandable 10Ks were provided again by G III Apparel Group LTD (cik = 821002, 21.82) but also Skechers USA Inc. (cik = 1065837, 22.91) and Fossil Group Inc. (cik= 883569, 22.92) were more understandable than the ohter 10Ks according to the FOG index. Surprisingly the company with the most complex 10Ks regarding the Flesch score, Lakeland Industries, achieves just a very average FOG score of 24.76.

References:
- Measuring Readability in Financial Disclosures; TIM LOUGHRAN and BILL MCDONALD; 2014
- Biddle, Gary, Gilles Hilary, and Rodrigo Verdi, 2009, How does financial reporting quality relate to investment efficiency? Journal of Accounting and Economics 48, 112–131.

```{r Readability plots FOG, echo = FALSE, fig.width = 10 , fig.fullwidth=TRUE}

# all companies, all years
ggplotly(ggplot(fashion_rd_fog, aes(x=fashion_10k$year, y =FOG, color=fashion_10k$cik)) + geom_point() + ggtitle("FOG - Readability for fashion companies"), tooltip = c("colour", "y", "x"))
```

In summary, both readability measures do calculate a very high complexity for all 10Ks with some differences on company-level. The trend of more difficult 10Ks towards recent years can be observed in both application of the two measures. However, it is highly questionable if these traditional readability measures can be applied to evaluate the readability of financial reports. Loughran and McDonald (2014) define the readability of financial reports as "the effective communication of valuation-relevant information". They conclude that just consider the file size of 10Ks represents a better estimator for the readability of the reports instead of commonly used readability measures. //TO DO// -> Measure the amount of tokens per company across years (only way to do it so far is by summarise the summary of the corpus and then plotting the result, but with summary we only get 100 observations, respecitvely 7 companies)

References:
- Measuring Readability in Financial Disclosures; TIM LOUGHRAN and BILL MCDONALD; 2014


#### Correspondence ####

```{r Correspondance analysis 2004 and 2020, include = FALSE}
# analysis only on one year across companies to have only max 27 data points (otherwise way too much)
# filter out the years 2004 and 2020 to show difference in correspondance
fashion_2004 <- fashion_10k %>%
  filter(year == 2004)
fashion_2020 <- fashion_10k %>%
  filter(year == 2020)

## create corpus
fashion_04_corp <- corpus(fashion_2004)
fashion_20_corp <- corpus(fashion_2020)

## Tokenization
fashion_04_tokens <- fashion_04_corp %>%
  tokens(remove_punct = TRUE, # remove punctuation
         remove_numbers = TRUE, # remove numbers
         remove_symbols = TRUE) %>% # remove symbols
  tokens_tolower() # everything to lower cases

fashion_04_tokens <- fashion_04_tokens %>%
  tokens_remove(stopwords("english"))

fashion_20_tokens <- fashion_20_corp %>%
  tokens(remove_punct = TRUE, # remove punctuation
         remove_numbers = TRUE, # remove numbers
         remove_symbols = TRUE) %>% # remove symbols
  tokens_tolower() # everything to lower cases

fashion_20_tokens <- fashion_20_tokens %>%
  tokens_remove(stopwords("english"))

## DFM
fashion_04_dfm <- dfm(fashion_04_tokens)
fashion_20_dfm <- dfm(fashion_20_tokens)

## 1D correspondence analysis
fashion_04_ca <- textmodel_ca(fashion_04_dfm)
fashion_20_ca <- textmodel_ca(fashion_20_dfm)

## 2D correspondence analysis
fashion_04_ca2 <- data.frame(dim1 = coef(fashion_04_ca, doc_dim = 1)$coef_document, 
                       dim2 = coef(fashion_04_ca, doc_dim = 2)$coef_document,
                       doc = fashion_04_ca$rownames)

fashion_20_ca2 <- data.frame(dim1 = coef(fashion_20_ca, doc_dim = 1)$coef_document, 
                       dim2 = coef(fashion_20_ca, doc_dim = 2)$coef_document,
                       doc = fashion_20_ca$rownames)
```

A conductued correspondence analysis supports the picture of higher differences in 10Ks in more recent years compared to current years as concluded by the readability measures. A graphical comparison of a two-dimensional correspondence analysis of the years 2004 and 2020 reveals a greater dispersion and thus greater differences in the 10K reports for the year 2004. This also provides an indication that a greater standardization of the filings could have taken place. However, some companies are always classified as different by the analysis over the years. Lakeland Industries Inc. (text7 in 2004 and text11 in 2020) shows a strong two-dimensional difference across all years. It is also the company which has the highest complexity according the Flesch score. Another company that stands out across multiple years is Unifi Inc. (text18 in 2004 and text24 in 2020). The 10Ks form Unifi seems to be (at least in one dimension) different from the others and different than the ones from Lakeland Industries.

```{r Plot 2D correspondance analysis, echo = FALSE, fig.width = 10 , fig.fullwidth=TRUE}
fashion_04_ca2 %>%
  ggplot(aes(x=dim1, y=dim2)) +
  geom_text(aes(label = doc)) +
  theme_classic() +
  labs(x="Dimension 1", y="Dimension 2")

fashion_20_ca2 %>%
  ggplot(aes(x=dim1, y=dim2)) +
  geom_text(aes(label = doc)) +
  theme_classic() +
  labs(x="Dimension 1", y="Dimension 2")
```

#### Collocation Analysis ####

#### Applied dictionaries ####

#### Relative ESG-wording ####

#### Comparison with ESG-Ratings ####

#### Trends between industries ####

#### Limitations ####
