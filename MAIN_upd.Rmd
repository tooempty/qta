---
title: "ESG Content Analysis on US companies"
author: "QTA group 2"
date: "5/19/2021"
output: html_document
---
## Table of Content (temporary)
Introduction // Sample selection and data collection // Research question // Readability and the nature of 10K fillings // Correspondence // Collocation Analysis // Applied dictionaries // Relative ESG-wording // Comparison with ESG-Ratings // Trends between industries // Limitations // Appendix

```{r Loading packages and load/prepare 10Ks, include=FALSE}
# load packages
library(readtext)
library(dplyr)
library(here)
library(readxl)
library(quanteda)
library(quanteda.textmodels)
library(stringr)
library(tidyverse)
library(ggplot2)
library(plotly)

# load excel files with cik numbers for each industry with the companies
fashion <- read_excel(here("fashion_cik.xlsx"), sheet = "Sheet1", col_names = TRUE)
fashion <- fashion[, c(1, 4:14)]

# energy <- read_excel()
# industry3 <- read_excel()

# keeping only the cik number
fashion_cik <- fashion$CIK
# energy_cik <- energy[,13]
# industry3_cik <- industry3[,13]

# load the 10Ks from downloaded files

fashion_10k <- NULL
energy_10k <- NULL
industry3_10k <- NULL

# loop that collects all reports for all companies for each industry
# this will take some minutes!!!

# fashion (should be 436 observations)
for (i in fashion_cik) {
  output <- readtext(here("Form10-K", i))
  fashion_10k <- rbind(fashion_10k, output)
}

# energy
for (i in energy_cik$CIK) {
  output <- readtext(here("Edgar filings_HTML view", "Form 10-K", i))
  fashion_10k <- rbind(all, output)
}

rm(output)

# industry3
for (i in industry_cik$CIK) {
  output <- readtext(here("Edgar filings_HTML view", "Form 10-K", i))
  fashion_10k <- rbind(all, output)
}

# simple cleaning
fashion_10k$text <- str_remove_all(fashion_10k$text, "\n•")
fashion_10k$text <- str_remove_all(fashion_10k$text, "\n")
#energy_10k
#industry3

# define columns into cik, report type and report year
fashion_10k <- separate(
  fashion_10k,
  1,
  into = c("cik", "type", "year", NA),
  sep = "_",
  remove = TRUE)

#energy
#industry3

# only keep year
#keep the year and not the full date
fashion_10k <- separate(
  fashion_10k,
  "year",
  into = c("year", NA),
  sep = "-",
  remove = TRUE)

#energy
#industry3
```


#### Introduction ####

#### Sample selection and data ####
We took the 10K filings back to the year 2004. This leads to 18 reports if the on from 2021 was already made public.For the fashion industry we have a average of `r round(ndoc(fashion_10k)/length(unique(fashion_10k$cik)), digits = 2)` total 10K filings per company. This is due to some later SEC registrations after 2004 for some companies.

"Financial reporting, or more precisely annual reports, are identified to be the most reliable disclosure to quantify a firm’s contribution to CSR." (Baier, Berninger and Kiesel (2020): Environmental, social and governance reporting in annual reports: A textual analysis)


#### Research question ####

#### Readability and the nature of 10K fillings ####
```{r Creating a corpus and apply the readability measures, include=FALSE}
# creating a corpus

fashion_corp <- corpus(fashion_10k)
#energy
#industry3


# apply Flesch readability measure (takes a few minutes)

fashion_rd_flesch <- fashion_corp %>%
  textstat_readability(measure = "Flesch", remove_hyphens = TRUE)
#energy
# industry3

# apply FOG readability measure (takes a few minutes)

fashion_rd_fog <- fashion_corp %>%
  textstat_readability(measure = "FOG", remove_hyphens = TRUE)
#energy
#industry3
```
//Flesch//

There are many different methods to try to measure the complexity of a underlying text source. Most measures take somehow into account the text structure, the length of sentences, the difficulty of used words and the usage of infrequent words. One of the first readability measures was developed by Rudolph Flesch back in 1948. As one of the oldest but still frequently used measure for readability it tries to reflect the complexity of a text by the average sentence length and the average number of syllables per word. The Flesch Reading Ease readability measure works with an index ranging from 0 to 100, with a higher index representing a simpler text to read. Theoretically a maximum score of 121.22 can be attended if every sentence just have a one-syllable word. There is also no lower limit. Therefore, some very complicated sentences can cause negative scores. With an average index of `r round(mean(fashion_rd_flesch$Flesch), digits = 2)` the 10Ks seem to be very complex based on the Flesch measure. A score below the score of 30 indicates a high difficulty to read the text and is best understood by university graduates. On the plot below the Flesch score for each company in our fashion sample across all years is shown. The readability scores tend to be more different between companies during the earlier years in our sample. From 2012 the companies have more similar reflected complexity as the scores vary much less as for the earlier ears. This can be due to more standardized restrictions by the SEC, but for the last two years 2020 and 2021 the scores do vary more again. Additionally, the complexity is increasing towards the present what is represented by a lower score. For the two last years multiple Flesch scores are below zero, suggesting these are very complicated texts to deal with. The higher variation and measured complexity could be caused by  COVID at how the firms are differently affected by the virus. Looking at the companies individually, it is noticeable that Lakeland Industries Inc. (cik = 798081, with an average Flesch score (AVS) of 5.81), Hansebrand Inc. (cik = 1359841, AVS = 7.65), PVH Corp. (cik = 78239, AVS = 9.06) and Carter Inc. (cik = 1060822, AVS = 9.37) tend to have more complex 10Ks. Whereas, the 10Ks from the companies G III Apparel Group LTD (cik = 821002, AVS = 19.92), Vera Bradley (cik = 1495320, AVS = 15.32) and Movado (cik = 72573, AVS = 14.34) are supposedly more easier to understand.

references:
- https://readabilityformulas.com/flesch-reading-ease-readability-formula.php


```{r Readability plots Flesch, echo = FALSE, fig.width = 10 , fig.fullwidth=TRUE}

# all companies, all years
ggplotly(ggplot(fashion_rd_flesch, aes(x=fashion_10k$year, y =Flesch, colour=fashion_10k$cik)) + geom_point() + ggtitle("Flesch - Readability for fashion companies"), tooltip = c("colour", "y", "x"))
```

//FOG//

Another well-known readability measure is the FOG index. It is widely used, also in the area of Accounting and Finance. Biddle, Hilary, and Verdi (2009) even stated that it is “a measure of financial statement readability”. The FOG index evaluates the difficulty of understanding a text on similar parameters like the Flesch index. Gunning-Fog, the creator of the FOG index used also the average sentence length. Together with the proportion of complex words (here words with 3-syllables) it forms a linear combination to measure the complexity of a underlying text source. In contrast to the Flesch score a higher FOG index indicates a more difficult text. A index of less than 8 is found to be understood by an universal audience, whereas a score above 17 can only be understood collage graduates. The average FOG score for our sample of 27 fashion companies from the US is `r round(mean(fashion_rd_fog$FOG), digits = 2)`. The 10Ks are therefore considered as very complex and difficult to read. The graph below displaying all readability scores for each fashion company over 18 years shows a similar picture than the one for the Flesch score. Keep in mind that the FOG scores behave the oppisite way than the Flesch scores, as a higher FOG score indicates a higher difficulty to read the text source. It can also be seen that the readability is lower towards the more recent years and the variance is slightly smaller during the years 2016 to 2019. However, it seems that for the FOG index the readability scores vary less, expect of single outliers. The company with the most complex 10Ks is PVH Corp. (cik = 78239, 26.11), which was the third complex one using the Flesch score. The most understandable 10Ks were provided again by G III Apparel Group LTD (cik = 821002, 21.82) but also Skechers USA Inc. (cik = 1065837, 22.91) and Fossil Group Inc. (cik= 883569, 22.92) were more understandable than the ohter 10Ks according to the FOG index. Surprisingly the company with the most complex 10Ks regarding the Flesch score, Lakeland Industries, achieves just a very average FOG score of 24.76.

References:
- Measuring Readability in Financial Disclosures; TIM LOUGHRAN and BILL MCDONALD; 2014
- Biddle, Gary, Gilles Hilary, and Rodrigo Verdi, 2009, How does financial reporting quality relate to investment efficiency? Journal of Accounting and Economics 48, 112–131.

```{r Readability plots FOG, echo = FALSE, fig.width = 10 , fig.fullwidth=TRUE}

# all companies, all years
ggplotly(ggplot(fashion_rd_fog, aes(x=fashion_10k$year, y =FOG, color=fashion_10k$cik)) + geom_point() + ggtitle("FOG - Readability for fashion companies"), tooltip = c("colour", "y", "x"))
```

In summary, both readability measures do calculate a very high complexity for all 10Ks with some differences on company-level. The trend of more difficult 10Ks towards recent years can be observed in both application of the two measures. However, it is highly questionable if these traditional readability measures can be applied to evaluate the readability of financial reports. Loughran and McDonald (2014) define the readability of financial reports as "the effective communication of valuation-relevant information". They conclude that just consider the file size of 10Ks represents a better estimator for the readability of the reports instead of commonly used readability measures. //TO DO// -> Measure the amount of tokens per company across years (only way to do it so far is by summarise the summary of the corpus and then plotting the result, but with summary we only get 100 observations, respecitvely 7 companies)

References:
- Measuring Readability in Financial Disclosures; TIM LOUGHRAN and BILL MCDONALD; 2014


####Correspondence ####

```{r Correspondance analysis 2004 and 2020, include = FALSE}
# analysis only on one year across companies to have only max 27 data points (otherwise way too much)
# filter out the years 2004 and 2020 to show difference in correspondance
fashion_2004 <- fashion_10k %>%
  filter(year == 2004)
fashion_2020 <- fashion_10k %>%
  filter(year == 2020)

## create corpus
fashion_04_corp <- corpus(fashion_2004)
fashion_20_corp <- corpus(fashion_2020)

## Tokenization
fashion_04_tokens <- fashion_04_corp %>%
  tokens(remove_punct = TRUE, # remove punctuation
         remove_numbers = TRUE, # remove numbers
         remove_symbols = TRUE) %>% # remove symbols
  tokens_tolower() # everything to lower cases

fashion_04_tokens <- fashion_04_tokens %>%
  tokens_remove(stopwords("english"))


A conductued correspondence analysis supports the picture of higher differences in 10Ks in more recent years compared to current years as concluded by the readability measures. A graphical comparison of a two-dimensional correspondence analysis of the years 2004 and 2020 reveals a greater dispersion and thus greater differences in the 10K reports for the year 2004. This also provides an indication that a greater standardization of the filings could have taken place. However, some companies are always classified as different by the analysis over the years. Lakeland Industries Inc. (text7 in 2004 and text11 in 2020) shows a strong two-dimensional difference across all years. It is also the company which has the highest complexity according the Flesch score. Another company that stands out across multiple years is Unifi Inc. (text18 in 2004 and text24 in 2020). The 10Ks form Unifi seems to be (at least in one dimension) different from the others and different than the ones from Lakeland Industries.

```{r Plot 2D correspondance analysis, echo = FALSE, fig.width = 10 , fig.fullwidth=TRUE}
fashion_04_ca2 %>%
  ggplot(aes(x=dim1, y=dim2)) +
  geom_text(aes(label = doc)) +
  theme_classic() +
  labs(x="Dimension 1", y="Dimension 2")

fashion_20_ca2 %>%
  ggplot(aes(x=dim1, y=dim2)) +
  geom_text(aes(label = doc)) +
  theme_classic() +
  labs(x="Dimension 1", y="Dimension 2")
```

#### Collocation Analysis ####

```{r Tokenization, include = FALSE}

fashion_tokens <- fashion_corp %>%
  tokens(remove_punct = TRUE, # remove punctuation
         remove_numbers = TRUE, # remove numbers
         remove_symbols = TRUE,
         remove_url = TRUE) %>% # remove symbols
  tokens_tolower() # everything to lower cases
# takes some minutes!

words_to_be_removed <- c(stopwords("english"), "million", "fiscal", "january", "february",
           "march", "april", "may", "june", "july", "august", "september",
           "october", "november", "december", "business", "net", "s")

# //KEEP IN MIND// to update the list in the appendix of removed words

fashion_tokens <- fashion_tokens %>%
  tokens_remove(words_to_be_removed)
```

#### Applied dictionaries ####


//CSR dict//

Pencle and Mălăescu (2016) developed a content analytic dictionary to measure the corporate social responsibility (CSR) of text documents issued by a company. The dictionary was composed by computer-assisted analyses of US IPOs between the years 2011 and 2013. Four dimensions should capture the different dimensions of CSR: Human rights with 297 entries, employee (319), social and community (361) and environment (451). It is notable that different words can occur in multiple dimensions, as for example the term "abuse" is contained in all four dimensions. //TO DO// -> table with dimensions, number of entries and typical examples of words included in the dictionary (human rights: civil_rights, fairness, freedom, minority or privileges; employee: benefit, disability, educate, equal, recognition, team or union; social and community: commitments, communal, foodbank, humans, protected, urban or volunteer; environment: emission, groundwater, renew, sustainable or waste).

If we remove all the stopwords and other terms that are insignificant for our analysis (see Appendix 1) we get a average of `r mean(fashion_count_tokens)` tokens for the fashion companies. Out of these a percentage of `r round(mean(ntoken(fashion_csr_all_dfm))/mean(count_tokens), digits = 4)*100`% tokens are regarded as CSR important terms according to the dictionary by Pencle and Mălăescu (2016). Looking at the different dimensions, the employee category provides the most amount of tokens and human rights contribute the least number of tokens. But all dimensions do provide a fair amount of tokens and the differences between them are small. In addition to the total amount of tokens, the frequency of which tokens appear how often plays a role. It is possible that a few terms occur extremely often and influence the results significantly. A wordcloud shows the most featured dictionary tokens for the fashion sector. It is noticeable that relatively common words such as "future", "management", "plan", "performance" and "employees" occur relatively often and can be significant drivers of the CSR wording.

reference:
- Pencle & Malescu (2016)

```{r Loading dictionaries, include = FALSE}

### Load CSR dict ###
dictionary_csr <- dictionary(file = "CorporateSocialResponsibility.cat")
# load the 1st dictionary CSR

fashion_CSR <- dfm(fashion_tokens, dictionary = dictionary_csr) # set up a dfm with word count by category

fashion_CSR <- convert(fashion_CSR, to = "data.frame") # convert it to df for easy merge

fashion_CSR <- cbind(fashion_CSR, fashion_10k[,c(1,3)]) # merge with cik and year data

count_tokens_fashion <- ntoken(fashion_tokens) #counts total tokens in the text
count_tokens_fashion <- as.data.frame(count_tokens_fashion) # convert to df for easy merge

fashion_CSR <- cbind(fashion_CSR, count_tokens_fashion) # merge with the token count
c# find the relative ESG-related words 

fashion_CSR_total <- fashion_CSR %>% 
        select(ENVIRONMENT, EMPLOYEE, 'HUMAN RIGHTS', 'SOCIAL AND COMMUNITY') %>% 
        rowSums(na.rm=TRUE)
  
fashion_CSR <- cbind(fashion_CSR, fashion_CSR_total) 

rm(fashion_CSR_total)
  
fashion_CSR <- fashion_CSR %>% mutate(relative_env = ENVIRONMENT/count_tokens_fashion,
                                              relative_soc = `SOCIAL AND COMMUNITY`/count_tokens_fashion,
                                              relative_empl = EMPLOYEE/count_tokens_fashion,
                                              relative_humans = `HUMAN RIGHTS`/count_tokens_fashion,
                                              relative_overall = fashion_CSR_total/count_tokens_fashion)

library(plotly)

```

```{r, include = TRUE}
plot_ly(data = fashion_CSR, x = ~year, y = ~relative_env, color = ~cik) %>% 
        add_markers() %>%
        layout(title = 'Share of environment wording. CSR dictionary',
              xaxis = list(title = 'year'),
              yaxis = list(title = 'percentage', hoverformat = '.2f'))

plot_ly(data = fashion_CSR, x = ~year, y = ~relative_soc, color = ~cik) %>% 
        add_markers() %>%
        layout(title = 'Share of social wording. CSR dictionary',
              xaxis = list(title = 'year'),
              yaxis = list(title = 'percentage', hoverformat = '.2f'))

plot_ly(data = fashion_CSR, x = ~year, y = ~relative_empl, color = ~cik) %>% 
        add_markers() %>%
        layout(title = 'Share of employee-related wording. CSR dictionary',
              xaxis = list(title = 'year'),
              yaxis = list(title = 'percentage', hoverformat = '.2f'))

plot_ly(data = fashion_CSR, x = ~year, y = ~relative_humans, color = ~cik) %>% 
       add_markers() %>%
        layout(title = 'Share of human-related wording. CSR dictionary',
              xaxis = list(title = 'year'),
              yaxis = list(title = 'percentage', hoverformat = '.2f'))

plot_ly(data = fashion_CSR, x = ~year, y = ~relative_overall, color = ~cik) %>% 
      add_markers() %>%      
      layout(title = 'Share of ESG wording. CSR dictionary',
              xaxis = list(title = 'year'),
              yaxis = list(title = 'percentage', hoverformat = '.2f'))

```

We further conduct the analysis on token level, and not aggregately, to see whether some ESG-related words words appear particularly frequently. 

```{r Wordcloud CSR dictionary, echo = FALSE}
### Select CSR dictionary tokens ###

## selecting tokens
fashion_csr_all_tokens <- tokens_select(fashion_tokens, dictionary_csr)
fashion_csr_hum_tokens <- tokens_select(fashion_tokens, dictionary_csr$`HUMAN RIGHTS`)
fashion_csr_emp_tokens <- tokens_select(fashion_tokens, dictionary_csr$EMPLOYEE)
fashion_csr_soc_tokens <- tokens_select(fashion_tokens, dictionary_csr$`SOCIAL AND COMMUNITY`)
fashion_csr_env_tokens <- tokens_select(fashion_tokens, dictionary_csr$ENVIRONMENT)

## creating dfm
fashion_dfm <- dfm(fashion_tokens)
fashion_csr_all_dfm <- dfm(fashion_csr_all_tokens)
fashion_csr_hum_dfm <- dfm(fashion_csr_hum_tokens)
fashion_csr_emp_dfm <- dfm(fashion_csr_emp_tokens)
fashion_csr_soc_dfm <- dfm(fashion_csr_soc_tokens)
fashion_csr_env_dfm <- dfm(fashion_csr_env_tokens)

## count token
fashion_count_tokens <- ntoken(fashion_dfm)
##

# wordcloud for most used words in the CSR dictionary
textplot_wordcloud(fashion_csr_all_dfm, max_words = 100,
                   color = c("grey80", "darkgoldenrod1", "tomato"))
```

//BBK dict //

With the increasing importance of environmental, social and governance (ESG) factors for companies and investors, Baier, Berninger and Kiesel (2020) have recently developed a new ESG content dictionary. The dictionary was trained on the basis of financial reports and is intended as a tool to reflect the ESG activities of companies. Each of the 482 words occurring in the dictionary is considered as a ESG-term that is mainly used in the context of ESG. The index consists of three main topics: governance, environmental and social. Further there are 11 categories and 35 subcategories that allows a very precise search for different aspects in the field of ESG.

reference:
- Baier, Berninger & Kiesel (2020)

```{r}

### Load BBK dict ###
dictionary_bbk <- read_excel(here("BaierBerningerKiesel.xlsx"), sheet = "Sheet1", col_names = TRUE)
# load the 2nd dictionary BBK Excel-file

Governance_BBK <- dictionary_bbk[dictionary_bbk$Topic == 'Governance',]
Governance_BBK <- Governance_BBK[1]

Environmental_BBK <- dictionary_bbk[dictionary_bbk$Topic == 'Environmental',]
Environmental_BBK <- Environmental_BBK[1]

Social_BBK <- dictionary_bbk[dictionary_bbk$Topic == 'Social',]
Social_BBK <- Social_BBK[1]

dictionary_BBK <- dictionary(list(Governance = Governance_BBK, 
                                  Environmental = Environmental_BBK, 
                                  Social = Social_BBK))

rm(Governance_BBK, Environmental_BBK, Social_BBK)

fashion_BBK <- dfm(fashion_tokens, dictionary = dictionary_BBK, valuetype = "glob") # set up a dfm with word count by category

fashion_BBK <- convert(fashion_BBK, to = "data.frame") # convert it to df for easy merge

fashion_BBK <- cbind(fashion_BBK, fashion_10k[,c(1,3)]) # merge with cik and year data
fashion_BBK <- cbind(fashion_BBK, count_tokens_fashion) # merge with the token count
# fashion_BBK$year <- as.Date(fashion_BBK$year, format="%d/%m/%Y") 

# find the relative ESG-related words 

total_ESG_only_BBK <- fashion_BBK %>% 
  select(Governance.Word, Environmental.Word, Social.Word) %>% 
  rowSums(na.rm=TRUE)

fashion_BBK <- cbind(fashion_BBK, total_ESG_only_BBK) 

fashion_BBK <- fashion_BBK %>% mutate(relative_govern = Governance.Word/count_tokens_fashion,
                                      relative_env = Environmental.Word/count_tokens_fashion,
                                      relative_soc = Social.Word/count_tokens_fashion,
                                      relative_overall = total_ESG_only_BBK/count_tokens_fashion)


plot_ly(data = fashion_BBK, x = ~year, y = ~relative_env, color = ~cik) %>% 
  add_markers() %>%
        layout(title = 'Share of environment wording. BBK dictionary',
              xaxis = list(title = 'year'),
              yaxis = list(title = 'percentage', hoverformat = '.2f'))

plot_ly(data = fashion_BBK, x = ~year, y = ~relative_soc, color = ~cik) %>% 
  add_markers() %>%
        layout(title = 'Share of social wording. BBK dictionary',
              xaxis = list(title = 'year'),
              yaxis = list(title = 'percentage', hoverformat = '.2f'))

plot_ly(data = fashion_BBK, x = ~year, y = ~relative_govern, color = ~cik) %>% 
  add_markers() %>%
        layout(title = 'Share of corporate governance wording. BBK dictionary',
              xaxis = list(title = 'year'),
              yaxis = list(title = 'percentage', hoverformat = '.2f'))

plot_ly(data = fashion_BBK, x = ~year, y = ~relative_overall, color = ~cik) %>% 
  add_markers() %>%
        layout(title = 'Share of ESG wording. CSR dictionary',
              xaxis = list(title = 'year'),
              yaxis = list(title = 'percentage', hoverformat = '.2f'))


## to-do: show only year, and not year and month........
```
#### Comparison with ESG-Ratings ####
Since our goal is to analyse whether 10-k reports carry valuable information regarding the company's ESG values, we further compare the ESG ratings received by the companies to the relative ESG sentiment from the 10-k report. 

```{r}
## 

```



#### Trends between industries ####

#### Limitations ####

#### Appendix ####

Appendix 1: Words that have been additionaly removed during the tokenization process due to insignificance.

words_to_be_removed <- c(stopwords("english"), "million", "fiscal", "january", "february",
           "march", "april", "may", "june", "july", "august", "september",
           "october", "november", "december", "business", "net", "s")
           

Appendix 2: Barplot of 75 most frequent dictionary tokens for the CSR dictionary

```{r Barplot of 75 most frequent dictionary tokens for the CSR dictionary, echo = FALSE}
ggplotly(ggplotly(ggplot(fashion_csr_all_top75_dfm, aes(x = feature, y = frequency))
         + geom_point(color="steelblue") +
           theme(axis.text.x = element_text(angle = 90, hjust = 1))))
```
